{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import torch\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QTextEdit, QLineEdit, QLabel\n",
    "from PyQt5.QtCore import Qt, QTimer\n",
    "import sys\n",
    "import threading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Seleziona il dispositivo: GPU, MPS, o CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # GPU NVIDIA\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # GPU Apple (M1/M2)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPU come fallback generico\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model (only one time we need to do this)\n",
    "\n",
    "# Carica il modello GPT-Neo e il tokenizer\n",
    "#model_name = \"EleutherAI/gpt-neo-1.3B\"  # Usa GPT-Neo 1.3B o la versione che preferisci\n",
    "#model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "#model_path = \"gpt-neo-1.3B_model\"\n",
    "\n",
    "# Salva il modello e il tokenizer nella cartella personalizzata\n",
    "#model.save_pretrained(model_path)\n",
    "#tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello e tokenizer caricati con successo!\n"
     ]
    }
   ],
   "source": [
    "# Carica il modello e il tokenizer\n",
    "model_path = \"gpt-neo-1.3B_model\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Modello e tokenizer caricati con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token  # Usa eos_token come pad_token\n",
    "\n",
    "# Funzione per generare risposte con opzioni migliorate\n",
    "def generate_response(prompt, max_length=75):\n",
    "    # Tokenizza l'input e sposta i tensori sul dispositivo (GPU o CPU)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, return_attention_mask=True).to(device)\n",
    "    \n",
    "    # Usa solo 'input_ids' per la generazione\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],  # Passa solo input_ids\n",
    "        attention_mask=inputs['attention_mask'],  # Passa l'attenzione mask\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1, \n",
    "        temperature=0.7, \n",
    "        top_p=0.5,\n",
    "        top_k=50,\n",
    "        repetition_penalty=5.0,\n",
    "        length_penalty=5.0,\n",
    "    )\n",
    "    \n",
    "    # Decodifica la risposta generata\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBotApp(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Chatbot GPT-Neo\")\n",
    "        self.setGeometry(100, 100, 600, 700)\n",
    "\n",
    "        # Layout verticale\n",
    "        self.layout = QVBoxLayout()\n",
    "\n",
    "        # Etichetta per il messaggio di attesa (casella fissa)\n",
    "        self.wait_label = QLabel('The answer may take a few seconds ...', self)\n",
    "        self.layout.addWidget(self.wait_label)\n",
    "\n",
    "        # Finestra di chat\n",
    "        self.chat_window = QTextEdit(self)\n",
    "        self.chat_window.setReadOnly(True)  # La finestra di chat è in sola lettura\n",
    "        self.layout.addWidget(self.chat_window)\n",
    "\n",
    "        # Casella di input per l'utente\n",
    "        self.user_input_box = QLineEdit(self)\n",
    "        self.layout.addWidget(self.user_input_box)\n",
    "\n",
    "        # Bottone per inviare il messaggio\n",
    "        self.send_button = QPushButton(\"Send\", self)\n",
    "        self.send_button.clicked.connect(self.send_message)\n",
    "        self.layout.addWidget(self.send_button)\n",
    "\n",
    "        # Imposta il layout\n",
    "        self.setLayout(self.layout)\n",
    "\n",
    "    def send_message(self):\n",
    "        user_input = self.user_input_box.text()\n",
    "        if user_input.strip():\n",
    "            # Mostra il messaggio dell'utente nella finestra di chat\n",
    "            self.chat_window.append(f'<b>You:</b> {user_input}')\n",
    "            \n",
    "            # Pulisci la casella di input\n",
    "            self.user_input_box.clear()\n",
    "\n",
    "            # Mostra la casella di attesa\n",
    "            self.wait_label.setText('La risposta potrebbe impiegarci qualche secondo...')\n",
    "\n",
    "            # Usa threading per evitare che il kernel crashi\n",
    "            threading.Thread(target=self.generate_and_display_response, args=(user_input,)).start()\n",
    "\n",
    "    def generate_and_display_response(self, user_input):\n",
    "        \"\"\"Genera la risposta del bot e la visualizza\"\"\"\n",
    "        # Genera la risposta dal modello GPT-Neo\n",
    "        response = generate_response(user_input)\n",
    "\n",
    "        # Rimuovi la casella di attesa e aggiungi la risposta del bot\n",
    "        self.wait_label.setText('')  # Rimuove il messaggio di attesa\n",
    "\n",
    "        # Aggiungi la risposta del bot nella chat\n",
    "        self.chat_window.append(f'<b>Bot:</b> {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 20:45:58.641 Python[13969:337501] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n",
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/transformers/generation/configuration_utils.py:657: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `5.0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "QObject::connect: Cannot queue arguments of type 'QTextCursor'\n",
      "(Make sure 'QTextCursor' is registered using qRegisterMetaType().)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = QApplication(sys.argv)\n",
    "window = ChatBotApp()\n",
    "window.show()\n",
    "app.exec_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
